\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{sec:environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{sec:stateSpaceIntro}{{1.2}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statespaceSymm}{{1(a)}{2}}
\newlabel{sub@fig:statespaceSymm}{{(a)}{2}}
\newlabel{fig:NewStateRep}{{1(b)}{2}}
\newlabel{sub@fig:NewStateRep}{{(b)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the symmetry and corresponding values of the new state space representation\relax }}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Colormap of $V$-values, the brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.}}}{2}}
\newlabel{fig:statespaceIll}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{2}}
\citation{RL1}
\citation{RL1}
\citation{RL1}
\citation{RL1}
\@writefile{toc}{\contentsline {section}{\numberline {2}Independent Q-Learning}{3}}
\newlabel{sec:IQL}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}State space}{3}}
\newlabel{sec:stateSpaceIQL}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Implementation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Results}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Using an 11 $\times $ 11 grid}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bla\relax }}{4}}
\newlabel{}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Using a 9 $\times $ 9 grid}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Error bar plot of performance in a 9 $\times $ 9 grid\relax }}{5}}
\newlabel{fig:meansSds}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}(S) Minimax-Q algorithm}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Minimax principle and translation to Markov Games}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Implementation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of steps after each sweep for the three matches\relax }}{6}}
\newlabel{tab:minimaxTable}{{1}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces V-values of Minimax Predator\relax }}{6}}
\newlabel{tab:predM}{{2}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces V-values of Minimax Prey\relax }}{7}}
\newlabel{tab:preyM}{{3}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class diagram}{8}}
\newlabel{app:classDiagram}{{A}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policies for Minimax-Q}{9}}
\newlabel{app:policiesM}{{B}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Policy for predator}{9}}
\newlabel{policyLabelPred}{{\caption@xref {policyLabelPred}{ on input line 14}}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Policy resulting from Minimax-Q for the predator\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Policy for prey}{10}}
\newlabel{policyLabelPrey}{{\caption@xref {policyLabelPrey}{ on input line 92}}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Policy resulting from Minimax-Q for the prey\relax }}{10}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{minimax}{1}
\bibcite{commonsmath}{2}
\bibcite{RL1}{3}
