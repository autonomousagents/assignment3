
\section{(S) Minimax-Q algorithm}
The Minimax-Q algorithm can be used for planning in a Markov Game. It is an implementation of the general Minimax principle for Markov Games such as the prey-predator scenario in this assignment. This algorithm needs a strictly competitive scenario and is therefore implemented for the one prey and one predator setting. 

\subsection{Minimax principle and translation to Markov Games}
The Minimax principle is stated in \cite{minimax} as: "Behave so as to maximize your reward in the worst case". This means that the policy should be such that the agent takes the action resulting in the highest expected reward under the assumption that the opponent will also take the action that results in the highest expected reward for the opponent.  \\
 
The value of a state $s\in S$ in a Markov Game is:
\begin{align*}
V(s) &= \operatorname*{arg\,max}_{\pi \in PD(A)}
\operatorname*{arg\,min}_{o \in O}
\sum_{a \in A} Q(s,a,o) \pi_a
\end{align*}
where
\begin{align*}
Q(s,a,o) &= R(s,a,o) + \gamma \sum_{s'} T(s,a,o,s')V(s')
\end{align*}
Choosing the action that maximizes the reward in the worst case is than the policy that is guaranteed to result in an expected score of $V$ no matter which action the opponent chooses, for the maximal value of $V$. This can be written down in constraints where there is a constraint for each action the opponent can take:
%  
\begin{align*}
\pi_1 \cdot Q(s,a_1,o_1)+ ... + \pi_n \cdot Q(s,a_n,o_1) \geq V\\
\pi_1 \cdot Q(s,a_1,o_2)+ ... + \pi_n \cdot Q(s,a_n,o_2) \geq V\\
...\\
\pi_1 \cdot Q(s,a_1,o_n)+ ... + \pi_n \cdot Q(s,a_n,o_n) \geq V\\
\end{align*}

Off course the probabilities of $\pi$ are bounded by the constraints that they cannot be negative and they need to add up to one. After adding those constraints:

\begin{align*}
\pi_1 + ... + \pi_n &= 1\\
\pi_1 \geq 0\\
...\\
\pi_n  \geq 0\\
\end{align*}

the maximal value for $V$ can be found for which all constraints hold using linear programming. The resulting values for $\pi$ for each state are the policy used by the agent.

\subsection{Implementation}
The Minimax-Q algorithm is implemented for this assignment using Linear Programming  methods from the ``Math'' component of the JAVA library Apache Commons \cite{commonsmath}. The agent keeps track of a table of $V$-values to be able to calculate $Q(s,a,o)$ in order to find the right values for $\pi$. This can be done a number of times (or sweeps as they are called) as in Value Iteration. The resulting policy is used to behave in the environment to either catch the prey or avoid the predator.

\subsection{Results}
Three "matches" have been held using the Minimax-Q algorithm. The first match is a Random Prey versus a Minimax Predator, the second one a Random Predator versus a Minimax Prey and finally a Minimax Prey versus a Minimax Predator. After each sweep of planning 25 runs in the actual environment are held and the number of steps necessary to catch the prey is recorded. This averaged can then be compared for each sweep and between the different matches. The results for the first five sweeps can be found in \ref{tab:minimaxTable}, the pattern found here does not change when increasing the number of sweeps. The $V$-values for the Minimax Predator can be found in \ref{tab:predM} and for the Minimax Prey in \ref{tab:preyM} .  The resulting policies for the predator and the prey can be found in Appendix \ref{app:policiesM}.\\

\begin{table}[htb]
\centering
\begin{tabular}{lccccc}
&1&2&3&4&5\\
Minimax Predator vs Random Prey & 9923 & 9432&9517&9545&9572\\
Random Predator vs Minimax Prey & 282& 5000000& 5000000& 5000000& 5000000\\
Minimax Predator vs Minimax Prey & 221& 5000000& 5000000& 5000000& 5000000\\
\end{tabular}
\caption{Number of steps after each sweep for the three matches}
\label{tab:minimaxTable}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{cccccc}
0,0000 &  &  &  &  & \\
10,0000 & 4,3644 &  &  &  & \\ 
4,3644 & 1,7596 & 0,6019 &  &  & \\
1,7596 & 0,6019 & 0,1624 & 0,0328 &  & \\ 
0,6019 & 0,1624 & 0,0328 & 0,0047 & 0,0005 & \\ 
0,0968 & 0,0328 & 0,0047 & 0,0005 & 0,0000 & 0,0000\\ 
\end{tabular}
\caption{V-values of Minimax Predator}
\label{tab:predM}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{cccccc}
0,0000 &  &  &  &  & \\ 
0,0000 & 0,0000 &  &  &  & \\ 
0,0000 & 0,0000 & 0,0000 &  &  & \\ 
0,0000 & 0,0000 & 0,0000 & 0,0000 &  & \\ 
0,0000 & 0,0000 & 0,0000 & 0,0000 & 0,0000 & \\ 
0,0000 & 0,0000 & 0,0000 & 0,0000 & 0,0000 & 0,0000\\ 
\end{tabular}
\caption{V-values of Minimax Prey}
\label{tab:preyM}
\end{table}


\FloatBarrier
\subsection*{}